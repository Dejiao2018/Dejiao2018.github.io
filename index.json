[{"authors":null,"categories":null,"content":"I\u0026rsquo;m an applied scientist at Amazon Web Services (AWS).\nI received my Ph.D. degree in the Electrical Engineering and Computer Science department from the University of Michigan in 2019, under the supervision of Prof. Laura Balzano.\nProf. Laura Balzano. My research mainly focuses on deep generative models, information theory in neural networks, neural network compression, subspace learning, compressed sensing, and non-convex optimization. -- ","date":1615645588,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1615645588,"objectID":"598b63dd58b43bce02403646f240cd3c","permalink":"https://dejiaozhang.com/author/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/admin/","section":"author","summary":"I\u0026rsquo;m an applied scientist at Amazon Web Services (AWS).\nI received my Ph.D. degree in the Electrical Engineering and Computer Science department from the University of Michigan in 2019, under the supervision of Prof. Laura Balzano.\nProf. Laura Balzano. My research mainly focuses on deep generative models, information theory in neural networks, neural network compression, subspace learning, compressed sensing, and non-convex optimization. -- ","tags":null,"title":"","type":"author"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"d41d8cd98f00b204e9800998ecf8427e","permalink":"https://dejiaozhang.com/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"author","summary":"","tags":null,"title":"Authors","type":"author"},{"authors":null,"categories":null,"content":"","date":1636952400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636952400,"objectID":"48a1924f97bed6e1c98335c3161e050c","permalink":"https://dejiaozhang.com/publication/vascl/","publishdate":"2021-11-15T00:00:00-05:00","relpermalink":"/publication/vascl/","section":"publication","summary":"Despite profound successes, contrastive representation learning relies on carefully designed data augmentations using domain specific knowledge. This challenge is magnified in natural language processing where no general rules exist for data augmentation due to the discrete nature of natural language. We tackle this challenge by presenting a Virtual augmentation Supported Contrastive Learning of sentence representations (VaSCL). Originating from the interpretation that data augmentation essentially constructs the neighborhoods of each training instance, we in turn utilize the neighborhood to generate effective data augmentations. Leveraging the large training batch size of contrastive learning, we approximate the neighborhood of an instance via its K-nearest in-batch neighbors in the representation space. We then define an instance discrimination task regarding this neighborhood and generate the virtual augmentation in an adversarial training manner. We access the performance of VaSCL on a wide range of downstream tasks, and set a new state-of-the-art for unsupervised sentence representation learning.    ","tags":["Unsupervised Clustering"],"title":"Virtual Augmentation Supported Contrastive Learning of Sentence Representations","type":"publication"},{"authors":null,"categories":null,"content":"","date":1629864000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1629864000,"objectID":"77714b57090c32c9f7dbedc919b80f0c","permalink":"https://dejiaozhang.com/publication/knowledgesurvey/","publishdate":"2021-08-25T00:00:00-04:00","relpermalink":"/publication/knowledgesurvey/","section":"publication","summary":"Pretrained Language Models (PLM) have established a new paradigm through learning informative contextualized representations on large-scale text corpus. This new paradigm has revolutionized the entire field of natural language processing, and set the new state-of-the-art performance for a wide variety of NLP tasks. However, though PLMs could store certain knowledge/facts from training corpus, their knowledge awareness is still far from satisfactory. To address this issue, integrating knowledge into PLMs have recently become a very active research area and a variety of approaches have been developed. In this paper, we provide a comprehensive survey of the literature on this emerging and fast-growing field - Knowledge Enhanced Pretrained Language Models (KE-PLMs). We introduce three taxonomies to categorize existing work. Besides, we also survey the various NLU and NLG applications on which KE-PLM has demonstrated superior performance over vanilla PLMs. Finally, we discuss challenges that face KE-PLMs and also promising directions for future research.","tags":["Unsupervised Clustering"],"title":"Knowledge Enhanced Pretrained Language Models: A Compreshensive Survey","type":"publication"},{"authors":null,"categories":null,"content":"","date":1629864000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1629864000,"objectID":"3ad99bfdb9c410a999db9befa6506847","permalink":"https://dejiaozhang.com/publication/continuallearning/","publishdate":"2021-08-25T00:00:00-04:00","relpermalink":"/publication/continuallearning/","section":"publication","summary":"Pretrained language models (PTLMs) are typically learned over a large, static corpus and further fine-tuned for various downstream tasks. However, when deployed in the real world, a PTLM-based model must deal with data from a new domain that deviates from what the PTLM was initially trained on, or newly emerged data that contains out-of-distribution information. In this paper, we study a lifelong language model pretraining challenge where a PTLM is continually updated so as to adapt to emerging data. Over a domain-incremental research paper stream and chronologically ordered tweet stream, we incrementally pretrain a PTLM with different continual learning algorithms, and keep track of the downstream task performance (after fine-tuning) to analyze its ability of acquiring new knowledge and preserving learned knowledge. Our experiments show continual learning algorithms improve knowledge preservation, with logit distillation being the most effective approach. We further show that continual pretraining improves generalization when training and testing data of downstream tasks are drawn from different time steps, but do not improve when they are from the same time steps. We believe our problem formulation, methods, and analysis will inspire future studies towards continual pretraining of language models.","tags":["Unsupervised Clustering"],"title":"Lifelong Pretraining: Continually Adapting Language Models to Emerging Corpora","type":"publication"},{"authors":null,"categories":null,"content":"","date":1629864000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1629864000,"objectID":"1985d2100469652ad3630d1f4f63e98a","permalink":"https://dejiaozhang.com/publication/pairsupcon/","publishdate":"2021-08-25T00:00:00-04:00","relpermalink":"/publication/pairsupcon/","section":"publication","summary":"Many recent successes in sentence representation learning have been achieved by simply fine-tuning on the Natural Language Inference (NLI) datasets with triplet loss or siamese loss. Nevertheless, they share a common weakness: sentences in a contradiction pair are not necessarily from different semantic categories. Therefore, optimizing the semantic entailment and contradiction reasoning objective alone is inadequate to capture the high-level semantic structure. The drawback is compounded by the fact that the vanilla siamese or triplet losses only learn from individual sentence pairs or triplets, which often suffer from bad local optima. In this paper, we propose PairSupCon, an instance discrimination based approach aiming to bridge semantic entailment and contradiction understanding with high-level categorical concept encoding. We evaluate PairSupCon on various downstream tasks that involve understanding sentence semantics at different granularities. We outperform the previous state-of-the-art method with 10%--13% averaged improvement on eight clustering tasks, and 5%--6% averaged improvement on seven semantic textual similarity (STS) tasks.   ","tags":["Unsupervised Clustering"],"title":"Pairwise Supervised Contrastive Learning of Sentence Representations","type":"publication"},{"authors":null,"categories":null,"content":"","date":1615352400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615650248,"objectID":"edf78f0aeb4436d0e16fc2a7644e6486","permalink":"https://dejiaozhang.com/publication/sccl/","publishdate":"2021-03-10T00:00:00-05:00","relpermalink":"/publication/sccl/","section":"publication","summary":"Unsupervised clustering aims at discovering the semantic categories of data according to some distance measure in the representation space. However, different categories often overlap with each other in the representation space at the beginning of the learning process, which poses a significant challenge for distance-based clustering in achieving good separation between different categories. To this end, we propose Supporting Clustering with Contrastive Learning (SCCL) -- a novel framework to leverage contrastive learning to promote better separation. We assess the performance of SCCL on short text clustering tasks and show that SCCL significantly advances the state-of-the-art results on six out of eight benchmark datasets with 3%-11% improvement on Accuracy and 4%-15% improvement on Normalized Mutual Information. Furthermore, our quantitative analysis demonstrates the effectiveness of SCCL in leveraging the strengths of both bottom-up instance discrimination and  top-down clustering to achieve better intra-cluster and inter-cluster distances when evaluated with the true labels.","tags":["Unsupervised Clustering"],"title":"Supporting Clustering with Contrastive Learning","type":"publication"},{"authors":null,"categories":null,"content":"","date":1593489600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615650640,"objectID":"5cd9ca8615e3638caa418bc9ffa11681","permalink":"https://dejiaozhang.com/publication/mdd/","publishdate":"2020-06-30T00:00:00-04:00","relpermalink":"/publication/mdd/","section":"publication","summary":"Unsupervised domain adaptation addresses the problem of leveraging labeled data in a source domain to learn a well-performing model in a target domain where labels are unavailable. In this paper, we follow recent theoretical work and adopt the Margin Disparity Discrepancy (MDD) unsupervised domain adaptation algorithm to solve the cross-lingual text labeling problems. Experiments on cross-lingual document classification and NER demonstrate the proposed domain adaptation approach achieves significant improvements over state-of-the-art results. We further improve MDD by efficiently optimizing the margin loss on the source domain via Virtual Adversarial Training (VAT). This bridges the gap between the theoretical results and the actually used loss function in the original MDD work, and hence boosting the performance remarkably. Our numerical results also indicate that VAT can generally improve the generalization performance of both domains for different domain adaptation approaches.","tags":["Transfer Learning"],"title":"Margin-aware Unsupervised Domain Adaptation for Cross-lingual Text Labeling","type":"publication"},{"authors":null,"categories":null,"content":"","date":1539057600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593296358,"objectID":"d2070e817f0d1cccf65c3952cde9ef10","permalink":"https://dejiaozhang.com/publication/grouse/","publishdate":"2018-10-09T00:00:00-04:00","relpermalink":"/publication/grouse/","section":"publication","summary":"Subspace learning and matrix factorization problems have great many applications in science and engineering, and efficient algorithms are critical as dataset sizes continue to grow. Many relevant problem formulations are non-convex, and in a variety of contexts it has been observed that solving the non-convex problem directly is not only efficient but reliably accurate. We discuss convergence theory for a particular method: first order incremental gradient descent constrained to the Grassmannian. The output of the algorithm is an orthonormal basis for a d-dimensional subspace spanned by an input streaming data matrix. We study two sampling cases: where each data vector of the streaming matrix is fully sampled, or where it is undersampled by a sampling matrix. Our results cover two cases, where $A_t$ is Gaussian or a subset of rows of the identity matrix. We propose an adaptive stepsize scheme that depends only on the sampled data and algorithm outputs. We prove that with fully sampled data, the stepsize scheme maximizes the improvement of our convergence metric at each iteration, and this method converges from any random initialization to the true subspace, despite the non-convex formulation and orthogonality constraints. For the case of undersampled data, we establish monotonic expected improvement on the defined convergence metric for each iteration with high probability.","tags":["SubspaceLearn"],"title":"Convergence of a Grassmannian Gradient Descent Algorithm for Subspace Estimation from Undersampled Data","type":"publication"},{"authors":null,"categories":null,"content":"","date":1525060800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593292110,"objectID":"92219f1de80481ebce5f9a220968f2ad","permalink":"https://dejiaozhang.com/publication/growl/","publishdate":"2018-04-30T00:00:00-04:00","relpermalink":"/publication/growl/","section":"publication","summary":"Deep neural networks (DNNs) may contain millions, even billions, of parameters/weights, making storage and computation very expensive and motivating a large body of work aimed at reducing their complexity by using, e.g., sparsity-inducing regularization. Parameter sharing/tying is another well-known approach for controlling the complexity of DNNs by forcing certain sets of weights to share a common value. Some forms of weight sharing are hard-wired to express certain invariances; a notable example is the shift-invariance of convolutional layers. However, other groups of weights may be tied together during the learning process to  further reduce the network complexity. In this paper, we adopt a recently proposed regularizer,  GrOWL (group ordered weighted L1), which encourages sparsity and, simultaneously, learns which groups of parameters should share a common value. GrOWL has been proven effective in linear regression, being able to identify and cope with strongly correlated covariates. Unlike standard sparsity-inducing regularizers (e.g., L1 a.k.a. Lasso), GrOWL not only eliminates unimportant neurons by setting all their weights to zero, but also explicitly identifies strongly correlated neurons by tying the corresponding weights to a common value. This ability of GrOWL motivates the following two-stage procedure: (i) use GrOWL regularization during training to simultaneously identify significant neurons and groups of parameters that should be tied together; (ii) retrain the network, enforcing the structure that was unveiled in the previous phase, i.e.,  keeping only the significant neurons and enforcing the learned tying structure. We evaluate this approach on several benchmark datasets, showing that it can dramatically compress the network with slight or even no loss on generalization accuracy.","tags":["NNCompression"],"title":"Learning to Share: Simultaneous Parameter Tying and Sparsification in Deep Learning","type":"publication"},{"authors":null,"categories":null,"content":"More detail can easily be written here using Markdown and $\\rm \\LaTeX$ math code.\n","date":1514178000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593292110,"objectID":"1e36bb5184074e5d89f98b32cc3abd5b","permalink":"https://dejiaozhang.com/publication/mixae/","publishdate":"2017-12-25T00:00:00-05:00","relpermalink":"/publication/mixae/","section":"publication","summary":"Unsupervised clustering is one of the most fundamental challenges in machine learning. A popular hypothesis is that data are generated from a union of low-dimensional nonlinear manifolds; thus an approach to clustering is identifying and separating these manifolds. In this paper, we present a novel approach to solve this problem by using a mixture of autoencoders. Our model consists of two parts: 1) a collection of autoencoders where each autoencoder learns the underlying manifold of a group of similar objects, and 2) a mixture assignment neural network, which takes the concatenated latent vectors from the autoencoders as input and infers the distribution over clusters. By jointly optimizing the two parts, we simultaneously assign data to clusters and learn the underlying manifolds of each cluster.","tags":["DeepClustering"],"title":"Deep Unsupervised Clustering Using Mixture of Autoencoders","type":"publication"},{"authors":null,"categories":null,"content":"","date":1490414400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615650248,"objectID":"81d4e525803cd5f5b5996edef7c71476","permalink":"https://dejiaozhang.com/publication/match/","publishdate":"2017-03-25T00:00:00-04:00","relpermalink":"/publication/match/","section":"publication","summary":"We consider the problem of detecting whether a high dimensional signal lies in a given low dimensional subspace using only a few compressive measurements of it. By leveraging modern random matrix theory, we show that, even when we are short on information, a reliable detector can be constructed via a properly defined measure of energy of the signal outside the subspace.","tags":["SubspaceLearn"],"title":"Matched Subspace Detection Using Compressively Sampled Data","type":"publication"},{"authors":null,"categories":null,"content":"","date":1462766400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593292110,"objectID":"3c91407a3899401ffcb7b5c06900e746","permalink":"https://dejiaozhang.com/publication/grouse_aistats/","publishdate":"2016-05-09T00:00:00-04:00","relpermalink":"/publication/grouse_aistats/","section":"publication","summary":"It has been observed in a variety of contexts that gradient descent methods have great success in solving low-rank matrix factorization problems, despite the relevant problem formulation being non-convex. We tackle a particular instance of this scenario, where we seek the d-dimensional subspace spanned by a streaming data matrix. We apply the natural first order incremental gradient descent method, constraining the gradient method to the Grassmannian. In this paper, we propose an adaptive step size scheme that is greedy for the noiseless case, that maximizes the improvement of our metric of convergence at each data index t, and yields an expected improvement for the noisy case. We show that, with noise-free data, this method converges from any random initialization to the global minimum of the problem. For noisy data, we provide the expected convergence rate of the proposed algorithm per iteration.","tags":["SubspaceLearn"],"title":"Global Convergence of a Grassmannian Gradient Descent Algorithm for Subspace Estimation","type":"publication"},{"authors":null,"categories":null,"content":"","date":1393736400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593292110,"objectID":"39c4dd89f8c1d1ced9993705f82cb037","permalink":"https://dejiaozhang.com/publication/tgrasta_journal/","publishdate":"2014-03-02T00:00:00-05:00","relpermalink":"/publication/tgrasta_journal/","section":"publication","summary":"Robust high-dimensional data processing has witnessed an exciting development in recent years, as theoretical results have shown that it is possible using convex programming to optimize data fit to a low-rank component plus a sparse outlier component. This problem is also known as Robust PCA, and it has found application in many areas of computer vision. In image and video processing and face  recognition, an exciting opportunity for processing of massive image databases is emerging as people upload photo and video data online in unprecedented volumes. However, the data quality and consistency is not controlled in any way, and the massiveness of the data poses a serious computational challenge. In this paper we present t-GRASTA (transformed Grassmannian Robust Adaptive Subspace Tracking Algorithm). t-GRASTA performs incremental gradient descent constrained to the Grassmann manifold of subspaces in order to simultaneously estimate a decomposition of a collection of images into a lowrank subspace, a sparse part of occlusions and foreground objects, and a transformation such as rotation or translation of the image. We show that t-GRASTA is 4 times faster than state-ofthe-art algorithms, has half the memory requirement, and can achieve alignment for face images as well as jittered camera surveillance images.","tags":["SubspaceLearn"],"title":"Iterative Grassmannian Optimization for Robust Image Alignment","type":"publication"},{"authors":null,"categories":null,"content":"More detail can easily be written here using Markdown and $\\rm \\LaTeX$ math code.\n","date":1368072000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593292110,"objectID":"ee8a9c5aa1cd2f1e51d4412e2c250295","permalink":"https://dejiaozhang.com/publication/tgrasta/","publishdate":"2013-05-09T00:00:00-04:00","relpermalink":"/publication/tgrasta/","section":"publication","summary":"Robust high-dimensional data processing has witnessed an exciting development in recent years, as theoretical results have shown that it is possible using convex programming to optimize data fit to a low-rank component plus a sparse outlier component. This problem is also known as Robust PCA, and it has found application in many areas of computer vision. In image and video processing and face  recognition, an exciting opportunity for processing of massive image databases is emerging as people upload photo and video data online in unprecedented volumes. However, the data quality and consistency is not controlled in any way, and the massiveness of the data poses a serious computational challenge. In this paper we present t-GRASTA (transformed Grassmannian Robust Adaptive Subspace Tracking Algorithm). t-GRASTA performs incremental gradient descent constrained to the Grassmann manifold of subspaces in order to simultaneously estimate a decomposition of a collection of images into a lowrank subspace, a sparse part of occlusions and foreground objects, and a transformation such as rotation or translation of the image. We show that t-GRASTA is 4 times faster than state-ofthe-art algorithms, has half the memory requirement, and can achieve alignment for face images as well as jittered camera surveillance images.","tags":["SubspaceLearn"],"title":"Iterative Online Subspace Learning for Robust Image Alignment","type":"publication"}]