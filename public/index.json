[{"authors":null,"categories":null,"content":"I\u0026rsquo;m an applied scientist at Amazon Web Services (AWS).\nI received my Ph.D. degree in the Electrical Engineering and Computer Science department from the University of Michigan in 2019, under the supervision of Prof. Laura Balzano.\nProf. Laura Balzano. My research mainly focuses on deep generative models, information theory in neural networks, neural network compression, subspace learning, compressed sensing, and non-convex optimization. -- ","date":1615645588,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1615645588,"objectID":"598b63dd58b43bce02403646f240cd3c","permalink":"https://dejiaozhang.com/author/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/admin/","section":"author","summary":"I\u0026rsquo;m an applied scientist at Amazon Web Services (AWS).\nI received my Ph.D. degree in the Electrical Engineering and Computer Science department from the University of Michigan in 2019, under the supervision of Prof. Laura Balzano.\nProf. Laura Balzano. My research mainly focuses on deep generative models, information theory in neural networks, neural network compression, subspace learning, compressed sensing, and non-convex optimization. -- ","tags":null,"title":"","type":"author"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"d41d8cd98f00b204e9800998ecf8427e","permalink":"https://dejiaozhang.com/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"author","summary":"","tags":null,"title":"Authors","type":"author"},{"authors":null,"categories":null,"content":"","date":1629864000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1629864000,"objectID":"1985d2100469652ad3630d1f4f63e98a","permalink":"https://dejiaozhang.com/publication/pairsupcon/","publishdate":"2021-08-25T00:00:00-04:00","relpermalink":"/publication/pairsupcon/","section":"publication","summary":"Many recent successes in sentence representation learning have been achieved by simply fine-tuning on the Natural Language Inference (NLI) datasets with triplet loss or siamese loss. Nevertheless, they share a common weakness: sentences in a contradiction pair are not necessarily from different semantic categories. Therefore, optimizing the semantic entailment and contradiction reasoning objective alone is inadequate to capture the high-level semantic structure. The drawback is compounded by the fact that the vanilla siamese or triplet losses only learn from individual sentence pairs or triplets, which often suffer from bad local optima. In this paper, we propose PairSupCon, an instance discrimination based approach aiming to bridge semantic entailment and contradiction understanding with high-level categorical concept encoding. We evaluate PairSupCon on various downstream tasks that involve understanding sentence semantics at different granularities. We outperform the previous state-of-the-art method with 10%--13% averaged improvement on eight clustering tasks, and 5%--6% averaged improvement on seven semantic textual similarity (STS) tasks.   ","tags":["Unsupervised Clustering"],"title":"Pairwise Supervised Contrastive Learning of Sentence Representations","type":"publication"},{"authors":null,"categories":null,"content":"","date":1615352400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615650248,"objectID":"edf78f0aeb4436d0e16fc2a7644e6486","permalink":"https://dejiaozhang.com/publication/sccl/","publishdate":"2021-03-10T00:00:00-05:00","relpermalink":"/publication/sccl/","section":"publication","summary":"Unsupervised clustering aims at discovering the semantic categories of data according to some distance measure in the representation space. However, different categories often overlap with each other in the representation space at the beginning of the learning process, which poses a significant challenge for distance-based clustering in achieving good separation between different categories. To this end, we propose Supporting Clustering with Contrastive Learning (SCCL) -- a novel framework to leverage contrastive learning to promote better separation. We assess the performance of SCCL on short text clustering tasks and show that SCCL significantly advances the state-of-the-art results on six out of eight benchmark datasets with 3%-11% improvement on Accuracy and 4%-15% improvement on Normalized Mutual Information. Furthermore, our quantitative analysis demonstrates the effectiveness of SCCL in leveraging the strengths of both bottom-up instance discrimination and  top-down clustering to achieve better intra-cluster and inter-cluster distances when evaluated with the true labels.","tags":["Unsupervised Clustering"],"title":"Supporting Clustering with Contrastive Learning","type":"publication"},{"authors":null,"categories":null,"content":"","date":1593489600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615650640,"objectID":"5cd9ca8615e3638caa418bc9ffa11681","permalink":"https://dejiaozhang.com/publication/mdd/","publishdate":"2020-06-30T00:00:00-04:00","relpermalink":"/publication/mdd/","section":"publication","summary":"Unsupervised domain adaptation addresses the problem of leveraging labeled data in a source domain to learn a well-performing model in a target domain where labels are unavailable. In this paper, we follow recent theoretical work and adopt the Margin Disparity Discrepancy (MDD) unsupervised domain adaptation algorithm to solve the cross-lingual text labeling problems. Experiments on cross-lingual document classification and NER demonstrate the proposed domain adaptation approach achieves significant improvements over state-of-the-art results. We further improve MDD by efficiently optimizing the margin loss on the source domain via Virtual Adversarial Training (VAT). This bridges the gap between the theoretical results and the actually used loss function in the original MDD work, and hence boosting the performance remarkably. Our numerical results also indicate that VAT can generally improve the generalization performance of both domains for different domain adaptation approaches.","tags":["Transfer Learning"],"title":"Margin-aware Unsupervised Domain Adaptation for Cross-lingual Text Labeling","type":"publication"},{"authors":null,"categories":null,"content":"","date":1543986000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615650499,"objectID":"14b52445f65fbb4d4ba83af1927b76a3","permalink":"https://dejiaozhang.com/publication/imae/","publishdate":"2018-12-05T00:00:00-05:00","relpermalink":"/publication/imae/","section":"publication","summary":"We propose the Information Maximization Autoencoder (IMAE), an information theoretic approach to simultaneously learn continuous and discrete representations in an unsupervised setting. Unlike the Variational Autoencoder framework, IMAE starts from a stochastic encoder that seeks to map each input data to a hybrid discrete and continuous representation with the objective of maximizing the mutual information between the data and their representations. A decoder is included to reconstruct the data given their representations, where a high fidelity reconstruction can be achieved by leveraging the informative representations. We show that the proposed objective is theoretically valid and provides a principled framework for understanding the tradeoffs regarding informativeness of each representation factor, disentanglement of representations, and decoding quality.","tags":["DeepGM"],"title":"Information Maximization Auto-Encoding","type":"publication"},{"authors":null,"categories":null,"content":"","date":1543986000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615650532,"objectID":"91bdc0e4fa89902796cb80b94ca1d37d","permalink":"https://dejiaozhang.com/publication/irnn/","publishdate":"2018-12-05T00:00:00-05:00","relpermalink":"/publication/irnn/","section":"publication","summary":"We propose the Information Maximization Autoencoder (IMAE), an information theoretic approach to simultaneously learn continuous and discrete representations in an unsupervised setting. Unlike the Variational Autoencoder framework, IMAE starts from a stochastic encoder that seeks to map each input data to a hybrid discrete and continuous representation with the objective of maximizing the mutual information between the data and their representations. A decoder is included to reconstruct the data given their representations, where a high fidelity reconstruction can be achieved by leveraging the informative representations. We show that the proposed objective is theoretically valid and provides a principled framework for understanding the tradeoffs regarding informativeness of each representation factor, disentanglement of representations, and decoding quality.","tags":["IRNN"],"title":"Information Regularized Neural Networks","type":"publication"},{"authors":null,"categories":null,"content":"","date":1539057600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593296358,"objectID":"d2070e817f0d1cccf65c3952cde9ef10","permalink":"https://dejiaozhang.com/publication/grouse/","publishdate":"2018-10-09T00:00:00-04:00","relpermalink":"/publication/grouse/","section":"publication","summary":"Subspace learning and matrix factorization problems have great many applications in science and engineering, and efficient algorithms are critical as dataset sizes continue to grow. Many relevant problem formulations are non-convex, and in a variety of contexts it has been observed that solving the non-convex problem directly is not only efficient but reliably accurate. We discuss convergence theory for a particular method: first order incremental gradient descent constrained to the Grassmannian. The output of the algorithm is an orthonormal basis for a d-dimensional subspace spanned by an input streaming data matrix. We study two sampling cases: where each data vector of the streaming matrix is fully sampled, or where it is undersampled by a sampling matrix. Our results cover two cases, where $A_t$ is Gaussian or a subset of rows of the identity matrix. We propose an adaptive stepsize scheme that depends only on the sampled data and algorithm outputs. We prove that with fully sampled data, the stepsize scheme maximizes the improvement of our convergence metric at each iteration, and this method converges from any random initialization to the true subspace, despite the non-convex formulation and orthogonality constraints. For the case of undersampled data, we establish monotonic expected improvement on the defined convergence metric for each iteration with high probability.","tags":["SubspaceLearn"],"title":"Convergence of a Grassmannian Gradient Descent Algorithm for Subspace Estimation from Undersampled Data","type":"publication"},{"authors":null,"categories":null,"content":"","date":1525060800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593292110,"objectID":"92219f1de80481ebce5f9a220968f2ad","permalink":"https://dejiaozhang.com/publication/growl/","publishdate":"2018-04-30T00:00:00-04:00","relpermalink":"/publication/growl/","section":"publication","summary":"Deep neural networks (DNNs) may contain millions, even billions, of parameters/weights, making storage and computation very expensive and motivating a large body of work aimed at reducing their complexity by using, e.g., sparsity-inducing regularization. Parameter sharing/tying is another well-known approach for controlling the complexity of DNNs by forcing certain sets of weights to share a common value. Some forms of weight sharing are hard-wired to express certain invariances; a notable example is the shift-invariance of convolutional layers. However, other groups of weights may be tied together during the learning process to  further reduce the network complexity. In this paper, we adopt a recently proposed regularizer,  GrOWL (group ordered weighted L1), which encourages sparsity and, simultaneously, learns which groups of parameters should share a common value. GrOWL has been proven effective in linear regression, being able to identify and cope with strongly correlated covariates. Unlike standard sparsity-inducing regularizers (e.g., L1 a.k.a. Lasso), GrOWL not only eliminates unimportant neurons by setting all their weights to zero, but also explicitly identifies strongly correlated neurons by tying the corresponding weights to a common value. This ability of GrOWL motivates the following two-stage procedure: (i) use GrOWL regularization during training to simultaneously identify significant neurons and groups of parameters that should be tied together; (ii) retrain the network, enforcing the structure that was unveiled in the previous phase, i.e.,  keeping only the significant neurons and enforcing the learned tying structure. We evaluate this approach on several benchmark datasets, showing that it can dramatically compress the network with slight or even no loss on generalization accuracy.","tags":["NNCompression"],"title":"Learning to Share: Simultaneous Parameter Tying and Sparsification in Deep Learning","type":"publication"},{"authors":null,"categories":null,"content":"","date":1520226000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615650248,"objectID":"b2bc06fe617ea12f5bc4689e647e5d04","permalink":"https://dejiaozhang.com/publication/adagrouse/","publishdate":"2018-03-05T00:00:00-05:00","relpermalink":"/publication/adagrouse/","section":"publication","summary":"This work investigates adaptive sampling strategies for online subspace estimation from streaming input vectors where the underlying subspace is coherent, i.e., aligned with some subset of the coordinate axes. We adapt the previously proposed Grassmannian rank-one update subspace estimation (GROUSE) algorithm to incorporate an adaptive sampling strategy that substantially improves over uniform random sampling. Our approach is to sample some proportion of the entries based on the leverage scores of the current subspace estimate. Experiments on synthetic data demonstrate that the adaptive measurement scheme greatly improves the convergence rate of GROUSE over uniform random measurements when the underlying subspace is coherent.","tags":["SubspaceLearn"],"title":"Online estimation of coherent subspaces with adaptive sampling","type":"publication"},{"authors":null,"categories":null,"content":"","date":1520226000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615650602,"objectID":"28c416c7a8836ff8a817522e9b98e1a8","permalink":"https://dejiaozhang.com/publication/owl/","publishdate":"2018-03-05T00:00:00-05:00","relpermalink":"/publication/owl/","section":"publication","summary":"A deep neural network (DNN) usually contains millions of parameters, making both storage and computation extremely expensive. Although this high capacity allows DNNs to learn sophisticated mappings, it also makes them prone to over- fitting. To tackle this issue, we adopt a recently proposed sparsity-inducing regularizer called OWL (ordered weighted L1, which has proven effective in sparse linear regression with strongly correlated covariates. Unlike the conventional sparsity-inducing regularizers, OWL simultaneously elimi- nates unimportant variables by setting their weights to zero, while also explicitly identifying correlated groups of variables by tying the corresponding weights to a common value. We evaluate the OWL regularizer on several deep learning bench- marks, showing that it can dramatically compress the network with slight or even no loss on generalization accuracy.","tags":["NNCompression"],"title":"Simultaneous sparsity and parameter tying for deep learning using ordered weighted L1 regularization","type":"publication"},{"authors":null,"categories":null,"content":"More detail can easily be written here using Markdown and $\\rm \\LaTeX$ math code.\n","date":1514178000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593292110,"objectID":"1e36bb5184074e5d89f98b32cc3abd5b","permalink":"https://dejiaozhang.com/publication/mixae/","publishdate":"2017-12-25T00:00:00-05:00","relpermalink":"/publication/mixae/","section":"publication","summary":"Unsupervised clustering is one of the most fundamental challenges in machine learning. A popular hypothesis is that data are generated from a union of low-dimensional nonlinear manifolds; thus an approach to clustering is identifying and separating these manifolds. In this paper, we present a novel approach to solve this problem by using a mixture of autoencoders. Our model consists of two parts: 1) a collection of autoencoders where each autoencoder learns the underlying manifold of a group of similar objects, and 2) a mixture assignment neural network, which takes the concatenated latent vectors from the autoencoders as input and infers the distribution over clusters. By jointly optimizing the two parts, we simultaneously assign data to clusters and learn the underlying manifolds of each cluster.","tags":["DeepClustering"],"title":"Deep Unsupervised Clustering Using Mixture of Autoencoders","type":"publication"},{"authors":null,"categories":null,"content":"More detail can easily be written here using Markdown and $\\rm \\LaTeX$ math code.\n","date":1512450000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593292110,"objectID":"a7742e4a98db83899a8ec40654c410c2","permalink":"https://dejiaozhang.com/publication/adagrouse_c/","publishdate":"2017-12-05T00:00:00-05:00","relpermalink":"/publication/adagrouse_c/","section":"publication","summary":"This work investigates the problem of adaptive measurement design for online subspace estimation from compressive linear measurements. We study the previously proposed Grassmannian rank-one online subspace estimation (GROUSE) algorithm with adaptively designed compressive measurements. We propose an adaptive measurement scheme that biases the measurement vectors towards the current subspace estimate and prove a global convergence result for the resulting algorithm. Our experiments on synthetic data demonstrate the effectiveness of the adaptive measurement scheme over non-adaptive compressive random measurements.","tags":["SubspaceLearn"],"title":"Enhanced Online Subspace Estimation via Adaptive Sensing","type":"publication"},{"authors":null,"categories":null,"content":"","date":1490414400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615650248,"objectID":"81d4e525803cd5f5b5996edef7c71476","permalink":"https://dejiaozhang.com/publication/match/","publishdate":"2017-03-25T00:00:00-04:00","relpermalink":"/publication/match/","section":"publication","summary":"We consider the problem of detecting whether a high dimensional signal lies in a given low dimensional subspace using only a few compressive measurements of it. By leveraging modern random matrix theory, we show that, even when we are short on information, a reliable detector can be constructed via a properly defined measure of energy of the signal outside the subspace.","tags":["SubspaceLearn"],"title":"Matched Subspace Detection Using Compressively Sampled Data","type":"publication"},{"authors":null,"categories":null,"content":"","date":1462766400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593292110,"objectID":"3c91407a3899401ffcb7b5c06900e746","permalink":"https://dejiaozhang.com/publication/grouse_aistats/","publishdate":"2016-05-09T00:00:00-04:00","relpermalink":"/publication/grouse_aistats/","section":"publication","summary":"It has been observed in a variety of contexts that gradient descent methods have great success in solving low-rank matrix factorization problems, despite the relevant problem formulation being non-convex. We tackle a particular instance of this scenario, where we seek the d-dimensional subspace spanned by a streaming data matrix. We apply the natural first order incremental gradient descent method, constraining the gradient method to the Grassmannian. In this paper, we propose an adaptive step size scheme that is greedy for the noiseless case, that maximizes the improvement of our metric of convergence at each data index t, and yields an expected improvement for the noisy case. We show that, with noise-free data, this method converges from any random initialization to the global minimum of the problem. For noisy data, we provide the expected convergence rate of the proposed algorithm per iteration.","tags":["SubspaceLearn"],"title":"Global Convergence of a Grassmannian Gradient Descent Algorithm for Subspace Estimation","type":"publication"},{"authors":null,"categories":null,"content":"","date":1393736400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593292110,"objectID":"39c4dd89f8c1d1ced9993705f82cb037","permalink":"https://dejiaozhang.com/publication/tgrasta_journal/","publishdate":"2014-03-02T00:00:00-05:00","relpermalink":"/publication/tgrasta_journal/","section":"publication","summary":"Robust high-dimensional data processing has witnessed an exciting development in recent years, as theoretical results have shown that it is possible using convex programming to optimize data fit to a low-rank component plus a sparse outlier component. This problem is also known as Robust PCA, and it has found application in many areas of computer vision. In image and video processing and face  recognition, an exciting opportunity for processing of massive image databases is emerging as people upload photo and video data online in unprecedented volumes. However, the data quality and consistency is not controlled in any way, and the massiveness of the data poses a serious computational challenge. In this paper we present t-GRASTA (transformed Grassmannian Robust Adaptive Subspace Tracking Algorithm). t-GRASTA performs incremental gradient descent constrained to the Grassmann manifold of subspaces in order to simultaneously estimate a decomposition of a collection of images into a lowrank subspace, a sparse part of occlusions and foreground objects, and a transformation such as rotation or translation of the image. We show that t-GRASTA is 4 times faster than state-ofthe-art algorithms, has half the memory requirement, and can achieve alignment for face images as well as jittered camera surveillance images.","tags":["SubspaceLearn"],"title":"Iterative Grassmannian Optimization for Robust Image Alignment","type":"publication"},{"authors":null,"categories":null,"content":"More detail can easily be written here using Markdown and $\\rm \\LaTeX$ math code.\n","date":1368072000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593292110,"objectID":"ee8a9c5aa1cd2f1e51d4412e2c250295","permalink":"https://dejiaozhang.com/publication/tgrasta/","publishdate":"2013-05-09T00:00:00-04:00","relpermalink":"/publication/tgrasta/","section":"publication","summary":"Robust high-dimensional data processing has witnessed an exciting development in recent years, as theoretical results have shown that it is possible using convex programming to optimize data fit to a low-rank component plus a sparse outlier component. This problem is also known as Robust PCA, and it has found application in many areas of computer vision. In image and video processing and face  recognition, an exciting opportunity for processing of massive image databases is emerging as people upload photo and video data online in unprecedented volumes. However, the data quality and consistency is not controlled in any way, and the massiveness of the data poses a serious computational challenge. In this paper we present t-GRASTA (transformed Grassmannian Robust Adaptive Subspace Tracking Algorithm). t-GRASTA performs incremental gradient descent constrained to the Grassmann manifold of subspaces in order to simultaneously estimate a decomposition of a collection of images into a lowrank subspace, a sparse part of occlusions and foreground objects, and a transformation such as rotation or translation of the image. We show that t-GRASTA is 4 times faster than state-ofthe-art algorithms, has half the memory requirement, and can achieve alignment for face images as well as jittered camera surveillance images.","tags":["SubspaceLearn"],"title":"Iterative Online Subspace Learning for Robust Image Alignment","type":"publication"}]